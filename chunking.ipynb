{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c80a95-5d07-4f6f-9ba2-db830e069f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "     \n",
    "Mounted at /content/drive\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "path = r'/content/drive/MyDrive/cleaned_subtitles.parquet'\n",
    "df = pd.read_parquet(path)\n",
    "     \n",
    "\n",
    "df.head(3)\n",
    "     \n",
    "num\tname\tcontent\n",
    "0\t9251120\tmaybe.this.time.(2014).eng.1cd\tWatch any video online with OpenSUBTITLES Free...\n",
    "1\t9211589\tdown.the.shore.s01.e10.and.justice.for.all.(19...\tOh I know that its getting late but I dont wan...\n",
    "2\t9380845\tuncontrollably.fond.s01.e07.heartache.(2016).e...\tTiming and Subtitles by The Uncontrollable Lov...\n",
    "Chunking Subtitle Text for Efficient Processing\n",
    "\n",
    "To improve text processing and search optimization, we split long subtitle texts into smaller, manageable chunks. This helps in better indexing, semantic search, and retrieval of relevant information.\n",
    "\n",
    "For chunking, we use LangChainâ€™s RecursiveCharacterTextSplitter, which ensures that text is divided into segments while maintaining meaningful context.\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents_langchain(df, chunk_size=512, chunk_overlap=100):\n",
    "    \"\"\"Chunks documents using Langchain and returns a new DataFrame with metadata.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "    for index, row in df.iterrows():\n",
    "        doc_chunks = text_splitter.split_text(row[\"content\"])\n",
    "        chunks.extend(doc_chunks)\n",
    "        # Store subtitle_id and subtitle_name in the metadata\n",
    "        metadatas.extend([{\"original_index\": index,\n",
    "                          \"subtitle_id\": row[\"num\"],\n",
    "                          \"subtitle_name\": row[\"name\"]}] * len(doc_chunks))\n",
    "\n",
    "    chunk_df = pd.DataFrame({\"chunk\": chunks, \"metadata\": metadatas})\n",
    "    return chunk_df\n",
    "     \n",
    "\n",
    "# Example usage (assuming 'new_df' has a 'content' column):\n",
    "chunked_df = chunk_documents_langchain(df)\n",
    "\n",
    "# Print the resulting DataFrame:\n",
    "print(chunked_df.head(3))\n",
    "     \n",
    "                                               chunk  \\\n",
    "0  Watch any video online with OpenSUBTITLES Free...   \n",
    "1  on Keep dancing Whatever Im kidding Dont get m...   \n",
    "2  And you Douche Handsome Conceited Just like yo...   \n",
    "\n",
    "                                            metadata  \n",
    "0  {'original_index': 0, 'subtitle_id': 9251120, ...  \n",
    "1  {'original_index': 0, 'subtitle_id': 9251120, ...  \n",
    "2  {'original_index': 0, 'subtitle_id': 9251120, ...  \n",
    "\n",
    "chunked_df.head()\n",
    "     \n",
    "chunk\tmetadata\n",
    "0\tWatch any video online with OpenSUBTITLES Free...\t{'original_index': 0, 'subtitle_id': 9251120, ...\n",
    "1\ton Keep dancing Whatever Im kidding Dont get m...\t{'original_index': 0, 'subtitle_id': 9251120, ...\n",
    "2\tAnd you Douche Handsome Conceited Just like yo...\t{'original_index': 0, 'subtitle_id': 9251120, ...\n",
    "3\tthat so Yes How long will this program run If ...\t{'original_index': 0, 'subtitle_id': 9251120, ...\n",
    "4\ther Gramps Uncle Erning Aunt Elma this is Tep ...\t{'original_index': 0, 'subtitle_id': 9251120, ...\n",
    "Saving Chunked Subtitle Data\n",
    "\n",
    "After splitting the subtitle content into smaller chunks, we need to store the processed data efficiently for future use. To do this, we follow these steps:\n",
    "\n",
    "Create a Directory for Storage\n",
    "\n",
    "We define a directory path /content/drive/MyDrive/search_engine/files/ to store the processed files.\n",
    "If the directory does not exist, it is created using os.makedirs().\n",
    "Save the Chunked Data in Parquet Format\n",
    "\n",
    "The chunked DataFrame is saved as subtitles_extracted.parquet using PyArrow for efficient storage.\n",
    "The Parquet format ensures better compression and faster read/write speeds.\n",
    "Handle Errors Gracefully\n",
    "\n",
    "A try-except block ensures that any errors during saving are caught and displayed.\n",
    "Once the file is successfully stored, it is ready for further processing, search indexing, and retrieval in the search engine pipeline.\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "path = \"/content/drive/MyDrive/search_engine/files/\"\n",
    "\n",
    "# Check if the directory exists, create it if it doesn't\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print(f\"Directory created: {path}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {path}\")\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "try:\n",
    "    chunked_df.to_parquet(f\"{path}subtitles_extracted.parquet\", engine=\"pyarrow\", index=False)\n",
    "    print(\"Successfully saved the dataset in:\", path)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "     \n",
    "Directory already exists: /content/drive/MyDrive/search_engine/files/\n",
    "Successfully saved the dataset in: /content/drive/MyDrive/search_engine/files/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
